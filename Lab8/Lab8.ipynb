{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\smain\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\smain\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\smain\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: click in c:\\users\\smain\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: tqdm in c:\\users\\smain\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\smain\\appdata\\roaming\\python\\python310\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.3; however, version 25.0.1 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\smain\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\smain\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (4.3.3)\n",
      "Requirement already satisfied: numpy<2.0,>=1.18.5 in c:\\users\\smain\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gensim) (1.26.4)\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in c:\\users\\smain\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gensim) (1.13.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\smain\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gensim) (7.1.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\smain\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.3; however, version 25.0.1 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\smain\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pattern in c:\\users\\smain\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.0.1a0)\n",
      "Requirement already satisfied: seaborn>=0.10.0 in c:\\users\\smain\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pattern) (0.13.2)\n",
      "Requirement already satisfied: scikit-learn>=1.5.0 in c:\\users\\smain\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pattern) (1.6.1)\n",
      "Requirement already satisfied: numpy>=1.26.4 in c:\\users\\smain\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pattern) (1.26.4)\n",
      "Requirement already satisfied: matplotlib>=3.9.0 in c:\\users\\smain\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pattern) (3.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\smain\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib>=3.9.0->pattern) (1.4.8)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\smain\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib>=3.9.0->pattern) (3.2.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\smain\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib>=3.9.0->pattern) (24.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\smain\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib>=3.9.0->pattern) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\smain\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib>=3.9.0->pattern) (0.12.1)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\smain\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib>=3.9.0->pattern) (11.1.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\smain\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib>=3.9.0->pattern) (4.56.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\smain\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib>=3.9.0->pattern) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\smain\\appdata\\roaming\\python\\python310\\site-packages (from python-dateutil>=2.7->matplotlib>=3.9.0->pattern) (1.17.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\smain\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn>=1.5.0->pattern) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\smain\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn>=1.5.0->pattern) (3.5.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\smain\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn>=1.5.0->pattern) (1.13.1)\n",
      "Requirement already satisfied: pandas>=1.2 in c:\\users\\smain\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from seaborn>=0.10.0->pattern) (2.2.3)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\smain\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas>=1.2->seaborn>=0.10.0->pattern) (2025.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\smain\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas>=1.2->seaborn>=0.10.0->pattern) (2025.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.3; however, version 25.0.1 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\smain\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sentence tokenizer:\n",
      "['Do you know how tokenization works?', \"It's actually quite interesting!\", \"Let's analyze a couple of sentences and figure it out.\"]\n",
      "\n",
      "Word tokenizer:\n",
      "['Do', 'you', 'know', 'how', 'tokenization', 'works', '?', 'It', \"'s\", 'actually', 'quite', 'interesting', '!', 'Let', \"'s\", 'analyze', 'a', 'couple', 'of', 'sentences', 'and', 'figure', 'it', 'out', '.']\n",
      "\n",
      "Word punct tokenizer:\n",
      "['Do', 'you', 'know', 'how', 'tokenization', 'works', '?', 'It', \"'\", 's', 'actually', 'quite', 'interesting', '!', 'Let', \"'\", 's', 'analyze', 'a', 'couple', 'of', 'sentences', 'and', 'figure', 'it', 'out', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, \\\n",
    "        word_tokenize, WordPunctTokenizer\n",
    "\n",
    "# Define input text\n",
    "input_text = \"Do you know how tokenization works? It's actually quite interesting! Let's analyze a couple of sentences and figure it out.\" \n",
    "\n",
    "# Sentence tokenizer \n",
    "print(\"\\nSentence tokenizer:\")\n",
    "print(sent_tokenize(input_text))\n",
    "\n",
    "# Word tokenizer\n",
    "print(\"\\nWord tokenizer:\")\n",
    "print(word_tokenize(input_text))\n",
    "\n",
    "# WordPunct tokenizer\n",
    "print(\"\\nWord punct tokenizer:\")\n",
    "print(WordPunctTokenizer().tokenize(input_text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce code utilise NLTK pour d√©couper un texte en phrases et en mots. Il commence par segmenter le texte en phrases avec sent_tokenize. Ensuite, il divise le texte en mots en tenant compte des apostrophes et de la ponctuation avec word_tokenize. Enfin, il applique WordPunctTokenizer, qui s√©pare les mots et la ponctuation de mani√®re plus stricte. Ce processus est utilis√© en traitement automatique du langage pour analyser et structurer du texte.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "       INPUT WORD          PORTER       LANCASTER        SNOWBALL \n",
      " ====================================================================\n",
      "         writing           write            writ           write\n",
      "          calves            calv            calv            calv\n",
      "              be              be              be              be\n",
      "         branded           brand           brand           brand\n",
      "           horse            hors            hors            hors\n",
      "       randomize          random          random          random\n",
      "        possibly         possibl            poss         possibl\n",
      "       provision          provis          provid          provis\n",
      "        hospital          hospit          hospit          hospit\n",
      "            kept            kept            kept            kept\n",
      "        scratchy        scratchi        scratchy        scratchi\n",
      "            code            code             cod            code\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "input_words = ['writing', 'calves', 'be', 'branded', 'horse', 'randomize', \n",
    "        'possibly', 'provision', 'hospital', 'kept', 'scratchy', 'code']\n",
    "\n",
    "# Create various stemmer objects\n",
    "porter = PorterStemmer()\n",
    "lancaster = LancasterStemmer()\n",
    "snowball = SnowballStemmer('english')\n",
    "\n",
    "# Create a list of stemmer names for display\n",
    "stemmer_names = ['PORTER', 'LANCASTER', 'SNOWBALL']\n",
    "formatted_text = '{:>16}' * (len(stemmer_names) + 1)\n",
    "print('\\n', formatted_text.format('INPUT WORD', *stemmer_names), \n",
    "        '\\n', '='*68)\n",
    "\n",
    "# Stem each word and display the output\n",
    "for word in input_words:\n",
    "    output = [word, porter.stem(word), \n",
    "            lancaster.stem(word), snowball.stem(word)]\n",
    "    print(formatted_text.format(*output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce code applique trois algorithmes de stemming (Porter, Lancaster et Snowball) √† une liste de mots en anglais. Il commence par cr√©er des objets pour chaque algorithme de stemming, puis d√©finit une liste de mots √† traiter. Ensuite, il g√©n√®re un tableau affichant chaque mot original et ses versions r√©duites selon chaque algorithme. Le stemming permet de r√©duire les mots √† leur racine pour normaliser le texte en traitement automatique du langage.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "               INPUT WORD         NOUN LEMMATIZER         VERB LEMMATIZER \n",
      " ===========================================================================\n",
      "                 writing                 writing                   write\n",
      "                  calves                    calf                   calve\n",
      "                      be                      be                      be\n",
      "                 branded                 branded                   brand\n",
      "                   horse                   horse                   horse\n",
      "               randomize               randomize               randomize\n",
      "                possibly                possibly                possibly\n",
      "               provision               provision               provision\n",
      "                hospital                hospital                hospital\n",
      "                    kept                    kept                    keep\n",
      "                scratchy                scratchy                scratchy\n",
      "                    code                    code                    code\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "input_words = ['writing', 'calves', 'be', 'branded', 'horse', 'randomize', \n",
    "        'possibly', 'provision', 'hospital', 'kept', 'scratchy', 'code']\n",
    "\n",
    "# Create lemmatizer object \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Create a list of lemmatizer names for display\n",
    "lemmatizer_names = ['NOUN LEMMATIZER', 'VERB LEMMATIZER']\n",
    "formatted_text = '{:>24}' * (len(lemmatizer_names) + 1)\n",
    "print('\\n', formatted_text.format('INPUT WORD', *lemmatizer_names), \n",
    "        '\\n', '='*75)\n",
    "\n",
    "# Lemmatize each word and display the output\n",
    "for word in input_words:\n",
    "    output = [word, lemmatizer.lemmatize(word, pos='n'),\n",
    "           lemmatizer.lemmatize(word, pos='v')]\n",
    "    print(formatted_text.format(*output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce code applique la lemmatisation √† une liste de mots en utilisant WordNetLemmatizer de NLTK. Contrairement au stemming, la lemmatisation ram√®ne un mot √† sa forme lexicale correcte en fonction de sa cat√©gorie grammaticale. Le script traite chaque mot sous deux formes : en tant que nom (pos='n') et en tant que verbe (pos='v'). Ensuite, il affiche un tableau comparant chaque mot avec ses versions lemmatis√©es. Ce processus est essentiel en traitement automatique du langage pour am√©liorer la pr√©cision de l'analyse s√©mantique.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of text chunks = 18 \n",
      "\n",
      "Chunk 1 ==> The Fulton County Grand Jury said Friday an invest\n",
      "Chunk 2 ==> '' . ( 2 ) Fulton legislators `` work with city of\n",
      "Chunk 3 ==> . Construction bonds Meanwhile , it was learned th\n",
      "Chunk 4 ==> , anonymous midnight phone calls and veiled threat\n",
      "Chunk 5 ==> Harris , Bexar , Tarrant and El Paso would be $451\n",
      "Chunk 6 ==> set it for public hearing on Feb. 22 . The proposa\n",
      "Chunk 7 ==> College . He has served as a border patrolman and \n",
      "Chunk 8 ==> of his staff were doing on the address involved co\n",
      "Chunk 9 ==> plan alone would boost the base to $5,000 a year a\n",
      "Chunk 10 ==> nursing homes In the area of `` community health s\n",
      "Chunk 11 ==> of its Angola policy prove harsh , there has been \n",
      "Chunk 12 ==> system which will prevent Laos from being used as \n",
      "Chunk 13 ==> reform in recipient nations . In Laos , the admini\n",
      "Chunk 14 ==> . He is not interested in being named a full-time \n",
      "Chunk 15 ==> said , `` to obtain the views of the general publi\n",
      "Chunk 16 ==> '' . Mr. Reama , far from really being retired , i\n",
      "Chunk 17 ==> making enforcement of minor offenses more effectiv\n",
      "Chunk 18 ==> to tell the people where he stands on the tax issu\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from nltk.corpus import brown\n",
    "\n",
    "# Split the input text into chunks, where\n",
    "# each chunk contains N words\n",
    "def chunker(input_data, N):\n",
    "    input_words = input_data.split(' ')\n",
    "    output = []\n",
    "\n",
    "    cur_chunk = []\n",
    "    count = 0\n",
    "    for word in input_words:\n",
    "        cur_chunk.append(word)\n",
    "        count += 1\n",
    "        if count == N:\n",
    "            output.append(' '.join(cur_chunk))\n",
    "            count, cur_chunk = 0, []\n",
    "\n",
    "    output.append(' '.join(cur_chunk))\n",
    "\n",
    "    return output \n",
    "\n",
    "if __name__=='__main__':\n",
    "    # Read the first 12000 words from the Brown corpus\n",
    "    input_data = ' '.join(brown.words()[:12000])\n",
    "\n",
    "    # Define the number of words in each chunk \n",
    "    chunk_size = 700\n",
    "\n",
    "    chunks = chunker(input_data, chunk_size)\n",
    "    print('\\nNumber of text chunks =', len(chunks), '\\n')\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        print('Chunk', i+1, '==>', chunk[:50])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce code divise un texte en segments de taille fixe en utilisant un chunking bas√© sur le nombre de mots. Il commence par extraire les 12 000 premiers mots du corpus Brown de NLTK, puis les segmente en morceaux contenant 700 mots chacun. Chaque chunk est ensuite stock√© dans une liste et affich√© avec ses 50 premiers caract√®res pour un aper√ßu. Ce type de segmentation est utile pour le traitement de texte en lots, notamment en NLP et apprentissage automatique.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Vocabulary:\n",
      " ['and' 'between' 'europe' 'formulate' 'have' 'in' 'mathematics' 'of'\n",
      " 'that' 'the']\n",
      "\n",
      "Document term matrix:\n",
      "\n",
      "         Word     Chunk-1     Chunk-2 \n",
      "\n",
      "         and           4           1\n",
      "     between           1           1\n",
      "      europe           1           1\n",
      "   formulate           1           1\n",
      "        have           1           1\n",
      "          in           6           1\n",
      " mathematics           1           1\n",
      "          of           5           3\n",
      "        that           2           1\n",
      "         the           9           1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Fonction pour lire le fichier\n",
    "def read_file(filepath):\n",
    "    with open(filepath, 'r', encoding='utf-8') as file:\n",
    "        return file.read()\n",
    "\n",
    "# Fonction pour diviser le texte en segments\n",
    "def chunker(text, chunk_size):\n",
    "    return [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "\n",
    "# Charger le texte depuis `data.txt`\n",
    "file_path = r\"C:\\Users\\smain\\OneDrive\\Documents\\data.txt\"  # Ajouter le `r` pour √©viter les probl√®mes d'√©chappement\n",
    "input_data = read_file(file_path)\n",
    "\n",
    "# Nombre de mots dans chaque chunk\n",
    "chunk_size = 800\n",
    "text_chunks = chunker(input_data, chunk_size)\n",
    "\n",
    "# Convertir les segments en dictionnaire\n",
    "chunks = [{'index': i, 'text': chunk} for i, chunk in enumerate(text_chunks)]\n",
    "\n",
    "# Extraction de la matrice terme-document\n",
    "count_vectorizer = CountVectorizer(min_df=2, max_df=20)  # Ajustez min_df si besoin\n",
    "document_term_matrix = count_vectorizer.fit_transform([chunk['text'] for chunk in chunks])\n",
    "\n",
    "# Extraction du vocabulaire\n",
    "vocabulary = np.array(count_vectorizer.get_feature_names_out())\n",
    "print(\"\\nVocabulary:\\n\", vocabulary)\n",
    "\n",
    "# G√©n√©rer les noms de chunks\n",
    "chunk_names = [f'Chunk-{i+1}' for i in range(len(text_chunks))]\n",
    "\n",
    "# Affichage de la matrice terme-document\n",
    "print(\"\\nDocument term matrix:\")\n",
    "formatted_text = '{:>12}' * (len(chunk_names) + 1)\n",
    "print('\\n', formatted_text.format('Word', *chunk_names), '\\n')\n",
    "\n",
    "for word, item in zip(vocabulary, document_term_matrix.T):\n",
    "    output = [word] + [str(freq) for freq in item.data]\n",
    "    print(formatted_text.format(*output))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce code lit un fichier texte (data.txt), le divise en segments de 800 caract√®res, puis cr√©e une matrice terme-document en utilisant CountVectorizer de sklearn. Il commence par charger le texte, le segmente en chunks, et construit un dictionnaire contenant ces segments. Ensuite, il extrait les termes fr√©quents (pr√©sents dans au moins 2 et au plus 20 segments) et g√©n√®re un vocabulaire. Enfin, il affiche la matrice terme-document qui repr√©sente la fr√©quence des mots dans chaque chunk. Ce proc√©d√© est utile en analyse de texte et NLP pour identifier les termes les plus significatifs d'un document volumineux.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Fetching training data...\n",
      "‚úÖ Training data loaded. Number of documents: 2844\n",
      "üõ† Vectorizing text data...\n",
      "‚úÖ Training data vectorized. Shape: (2844, 40018)\n",
      "üîÅ Transforming with TF-IDF...\n",
      "‚úÖ TF-IDF transformation done. Shape: (2844, 40018)\n",
      "ü§ñ Training Na√Øve Bayes model...\n",
      "‚úÖ Model trained successfully!\n",
      "üì° Transforming input data...\n",
      "üîÆ Predicting categories...\n",
      "\n",
      "üéØ Predictions:\n",
      "üîπ Input: You need to be careful with cars when you are driving on slippery roads\n",
      "   Predicted category: Autos\n",
      "\n",
      "üîπ Input: A lot of devices can be operated wirelessly\n",
      "   Predicted category: Electronics\n",
      "\n",
      "üîπ Input: Players need to be careful when they are close to goal posts\n",
      "   Predicted category: Hockey\n",
      "\n",
      "üîπ Input: Political debates help us understand the perspectives of both sides\n",
      "   Predicted category: Medicine\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n",
    "\n",
    "# D√©finition des cat√©gories\n",
    "category_map = {\n",
    "    'talk.politics.misc': 'Politics', \n",
    "    'rec.autos': 'Autos', \n",
    "    'rec.sport.hockey': 'Hockey', \n",
    "    'sci.electronics': 'Electronics', \n",
    "    'sci.med': 'Medicine'\n",
    "}\n",
    "\n",
    "print(\"üöÄ Fetching training data...\")\n",
    "training_data = fetch_20newsgroups(subset='train', categories=category_map.keys(), shuffle=True, random_state=5)\n",
    "print(f\"‚úÖ Training data loaded. Number of documents: {len(training_data.data)}\")\n",
    "\n",
    "# V√©rification si les donn√©es sont bien r√©cup√©r√©es\n",
    "if len(training_data.data) == 0:\n",
    "    print(\"‚ùå Aucune donn√©e r√©cup√©r√©e. V√©rifiez les cat√©gories !\")\n",
    "    exit()\n",
    "\n",
    "# Vectorisation avec suppression des stopwords\n",
    "print(\"üõ† Vectorizing text data...\")\n",
    "count_vectorizer = CountVectorizer(stop_words='english')\n",
    "train_tc = count_vectorizer.fit_transform(training_data.data)\n",
    "print(f\"‚úÖ Training data vectorized. Shape: {train_tc.shape}\")\n",
    "\n",
    "# V√©rification si la vectorisation fonctionne\n",
    "if train_tc.shape[0] == 0 or train_tc.shape[1] == 0:\n",
    "    print(\"‚ùå La matrice de compte est vide. Probl√®me de vectorisation !\")\n",
    "    exit()\n",
    "\n",
    "# Transformation TF-IDF\n",
    "print(\"üîÅ Transforming with TF-IDF...\")\n",
    "tfidf = TfidfTransformer()\n",
    "train_tfidf = tfidf.fit_transform(train_tc)\n",
    "print(f\"‚úÖ TF-IDF transformation done. Shape: {train_tfidf.shape}\")\n",
    "\n",
    "# Entra√Ænement du mod√®le Na√Øve Bayes\n",
    "print(\"ü§ñ Training Na√Øve Bayes model...\")\n",
    "classifier = MultinomialNB().fit(train_tfidf, training_data.target)\n",
    "print(\"‚úÖ Model trained successfully!\")\n",
    "\n",
    "# Donn√©es de test personnalis√©es\n",
    "input_data = [\n",
    "    'You need to be careful with cars when you are driving on slippery roads', \n",
    "    'A lot of devices can be operated wirelessly',\n",
    "    'Players need to be careful when they are close to goal posts',\n",
    "    'Political debates help us understand the perspectives of both sides'\n",
    "]\n",
    "\n",
    "# Transformation et pr√©diction\n",
    "print(\"üì° Transforming input data...\")\n",
    "input_tc = count_vectorizer.transform(input_data)\n",
    "input_tfidf = tfidf.transform(input_tc)\n",
    "\n",
    "print(\"üîÆ Predicting categories...\")\n",
    "predictions = classifier.predict(input_tfidf)\n",
    "\n",
    "# Affichage des r√©sultats\n",
    "print(\"\\nüéØ Predictions:\")\n",
    "for sent, category in zip(input_data, predictions):\n",
    "    print(f\"üîπ Input: {sent}\\n   Predicted category: {category_map[training_data.target_names[category]]}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce code entra√Æne un mod√®le Na√Øve Bayes multinomial pour classer des textes en fonction de leur contenu, en utilisant le dataset 20 Newsgroups. Il commence par r√©cup√©rer des articles li√©s √† cinq cat√©gories (politique, automobile, hockey, √©lectronique et m√©decine). Ensuite, il vectorise les textes en appliquant une transformation TF-IDF pour pond√©rer les mots les plus significatifs. Une fois le mod√®le entra√Æn√©, il est utilis√© pour pr√©dire la cat√©gorie de nouveaux textes donn√©s en entr√©e. L'affichage final montre chaque phrase test avec sa cat√©gorie pr√©dite, ce qui est utile pour l'analyse de texte et la classification automatique.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of end letters: 1\n",
      "Accuracy = 74.7%\n",
      "Alexander ==> male\n",
      "Danielle ==> female\n",
      "David ==> male\n",
      "Cheryl ==> male\n",
      "\n",
      "Number of end letters: 2\n",
      "Accuracy = 78.79%\n",
      "Alexander ==> male\n",
      "Danielle ==> female\n",
      "David ==> male\n",
      "Cheryl ==> female\n",
      "\n",
      "Number of end letters: 3\n",
      "Accuracy = 77.22%\n",
      "Alexander ==> male\n",
      "Danielle ==> female\n",
      "David ==> male\n",
      "Cheryl ==> female\n",
      "\n",
      "Number of end letters: 4\n",
      "Accuracy = 69.98%\n",
      "Alexander ==> male\n",
      "Danielle ==> female\n",
      "David ==> male\n",
      "Cheryl ==> female\n",
      "\n",
      "Number of end letters: 5\n",
      "Accuracy = 64.63%\n",
      "Alexander ==> male\n",
      "Danielle ==> female\n",
      "David ==> male\n",
      "Cheryl ==> female\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "from nltk import NaiveBayesClassifier\n",
    "from nltk.classify import accuracy as nltk_accuracy\n",
    "from nltk.corpus import names\n",
    "\n",
    "# Extract last N letters from the input word\n",
    "# and that will act as our \"feature\"\n",
    "def extract_features(word, N=2):\n",
    "    last_n_letters = word[-N:]\n",
    "    return {'feature': last_n_letters.lower()}\n",
    "\n",
    "if __name__=='__main__':\n",
    "    # Create training data using labeled names available in NLTK\n",
    "    male_list = [(name, 'male') for name in names.words('male.txt')]\n",
    "    female_list = [(name, 'female') for name in names.words('female.txt')]\n",
    "    data = (male_list + female_list)\n",
    "\n",
    "    # Seed the random number generator\n",
    "    random.seed(5)\n",
    "\n",
    "    # Shuffle the data\n",
    "    random.shuffle(data)\n",
    "\n",
    "    # Create test data\n",
    "    input_names = ['Alexander', 'Danielle', 'David', 'Cheryl']\n",
    "\n",
    "    # Define the number of samples used for train and test\n",
    "    num_train = int(0.8 * len(data))\n",
    "\n",
    "    # Iterate through different lengths to compare the accuracy\n",
    "    for i in range(1, 6):\n",
    "        print('\\nNumber of end letters:', i)\n",
    "        features = [(extract_features(n, i), gender) for (n, gender) in data]\n",
    "        train_data, test_data = features[:num_train], features[num_train:]\n",
    "        classifier = NaiveBayesClassifier.train(train_data)\n",
    "\n",
    "        # Compute the accuracy of the classifier \n",
    "        accuracy = round(100 * nltk_accuracy(classifier, test_data), 2)\n",
    "        print('Accuracy = ' + str(accuracy) + '%')\n",
    "\n",
    "        # Predict outputs for input names using the trained classifier model\n",
    "        for name in input_names:\n",
    "            print(name, '==>', classifier.classify(extract_features(name, i)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce code utilise un classificateur Na√Øve Bayes pour pr√©dire le genre d‚Äôun pr√©nom en fonction de ses derni√®res lettres. Il commence par extraire les pr√©noms masculins et f√©minins du corpus names de NLTK, puis les m√©lange de mani√®re al√©atoire. Ensuite, il entra√Æne un mod√®le sur 80 % des donn√©es et teste sa pr√©cision sur les 20 % restants. Il r√©p√®te ce processus pour des longueurs de suffixes allant de 1 √† 5 lettres afin de comparer l‚Äôimpact sur la pr√©cision. Enfin, il utilise le mod√®le entra√Æn√© pour pr√©dire le genre de nouveaux pr√©noms, comme Alexander ou Danielle. Ce type de classification est souvent utilis√© en NLP et analyse de donn√©es linguistiques.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of training datapoints: 1600\n",
      "Number of test datapoints: 400\n",
      "\n",
      "Accuracy of the classifier: 0.735\n",
      "\n",
      "Top 15 most informative words:\n",
      "1. outstanding\n",
      "2. insulting\n",
      "3. vulnerable\n",
      "4. ludicrous\n",
      "5. uninvolving\n",
      "6. astounding\n",
      "7. avoids\n",
      "8. fascination\n",
      "9. affecting\n",
      "10. animators\n",
      "11. anna\n",
      "12. darker\n",
      "13. seagal\n",
      "14. symbol\n",
      "15. idiotic\n",
      "\n",
      "Movie review predictions:\n",
      "\n",
      "Review: The costumes in this movie were great\n",
      "Predicted sentiment: Positive\n",
      "Probability: 0.59\n",
      "\n",
      "Review: I think the story was terrible and the characters were very weak\n",
      "Predicted sentiment: Negative\n",
      "Probability: 0.8\n",
      "\n",
      "Review: People say that the director of the movie is amazing\n",
      "Predicted sentiment: Positive\n",
      "Probability: 0.6\n",
      "\n",
      "Review: This is such an idiotic movie. I will not recommend it to anyone.\n",
      "Predicted sentiment: Negative\n",
      "Probability: 0.87\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import movie_reviews \n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.classify.util import accuracy as nltk_accuracy\n",
    " \n",
    "# Extract features from the input list of words\n",
    "def extract_features(words):\n",
    "    return dict([(word, True) for word in words])\n",
    " \n",
    "if __name__=='__main__':\n",
    "    # Load the reviews from the corpus \n",
    "    fileids_pos = movie_reviews.fileids('pos')\n",
    "    fileids_neg = movie_reviews.fileids('neg')\n",
    "     \n",
    "    # Extract the features from the reviews\n",
    "    features_pos = [(extract_features(movie_reviews.words(\n",
    "            fileids=[f])), 'Positive') for f in fileids_pos]\n",
    "    features_neg = [(extract_features(movie_reviews.words(\n",
    "            fileids=[f])), 'Negative') for f in fileids_neg]\n",
    "     \n",
    "    # Define the train and test split (80% and 20%)\n",
    "    threshold = 0.8\n",
    "    num_pos = int(threshold * len(features_pos))\n",
    "    num_neg = int(threshold * len(features_neg))\n",
    "     \n",
    "     # Create training and training datasets\n",
    "    features_train = features_pos[:num_pos] + features_neg[:num_neg]\n",
    "    features_test = features_pos[num_pos:] + features_neg[num_neg:]  \n",
    "\n",
    "    # Print the number of datapoints used\n",
    "    print('\\nNumber of training datapoints:', len(features_train))\n",
    "    print('Number of test datapoints:', len(features_test))\n",
    "     \n",
    "    # Train a Naive Bayes classifier \n",
    "    classifier = NaiveBayesClassifier.train(features_train)\n",
    "    print('\\nAccuracy of the classifier:', nltk_accuracy(\n",
    "            classifier, features_test))\n",
    "\n",
    "    N = 15\n",
    "    print('\\nTop ' + str(N) + ' most informative words:')\n",
    "    for i, item in enumerate(classifier.most_informative_features()):\n",
    "        print(str(i+1) + '. ' + item[0])\n",
    "        if i == N - 1:\n",
    "            break\n",
    "\n",
    "    # Test input movie reviews\n",
    "    input_reviews = [\n",
    "        'The costumes in this movie were great', \n",
    "        'I think the story was terrible and the characters were very weak',\n",
    "        'People say that the director of the movie is amazing', \n",
    "        'This is such an idiotic movie. I will not recommend it to anyone.' \n",
    "    ]\n",
    "\n",
    "    print(\"\\nMovie review predictions:\")\n",
    "    for review in input_reviews:\n",
    "        print(\"\\nReview:\", review)\n",
    "\n",
    "        # Compute the probabilities\n",
    "        probabilities = classifier.prob_classify(extract_features(review.split()))\n",
    "\n",
    "        # Pick the maximum value\n",
    "        predicted_sentiment = probabilities.max()\n",
    "\n",
    "        # Print outputs\n",
    "        print(\"Predicted sentiment:\", predicted_sentiment)\n",
    "        print(\"Probability:\", round(probabilities.prob(predicted_sentiment), 2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce code entra√Æne un classificateur Na√Øve Bayes pour effectuer une analyse de sentiment sur des critiques de films. Il commence par charger les critiques positives et n√©gatives du corpus movie_reviews de NLTK, puis extrait leurs caract√©ristiques en repr√©sentant chaque mot comme une caract√©ristique binaire (pr√©sent ou non). Ensuite, il divise les donn√©es en 80 % pour l'entra√Ænement et 20 % pour le test. Une fois le mod√®le entra√Æn√©, il affiche sa pr√©cision et identifie les 15 mots les plus informatifs pour la classification. Enfin, il teste le mod√®le sur de nouvelles critiques de films et pr√©dit leur sentiment positif ou n√©gatif avec un score de probabilit√©. Ce type d'analyse est utilis√© en NLP et opinion mining pour d√©tecter les √©motions dans les textes.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ 10 lignes charg√©es depuis 'C:\\Users\\smain\\OneDrive\\Documents\\data.txt'\n",
      "üîÑ Entra√Ænement du mod√®le LDA...\n",
      "‚úÖ Mod√®le LDA entra√Æn√© !\n",
      "\n",
      "üìå Top 5 mots cl√©s pour chaque sujet :\n",
      "\n",
      "üü¢ Sujet 1:\n",
      "  empir ==> 3.89%\n",
      "  mathemat ==> 3.89%\n",
      "  time ==> 2.78%\n",
      "  histor ==> 2.78%\n",
      "  peopl ==> 2.78%\n",
      "\n",
      "üü¢ Sujet 2:\n",
      "  europ ==> 3.12%\n",
      "  cultur ==> 3.12%\n",
      "  formul ==> 3.12%\n",
      "  set ==> 1.88%\n",
      "  structur ==> 1.88%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\smain\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer  \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from gensim import models, corpora\n",
    "\n",
    "# T√©l√©charger les stopwords si n√©cessaire\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# D√©finition du fichier\n",
    "input_file = r\"C:\\Users\\smain\\OneDrive\\Documents\\data.txt\"  # Chemin absolu Windows\n",
    "\n",
    "# V√©rifier si le fichier existe\n",
    "if not os.path.exists(input_file):\n",
    "    print(f\"‚ùå Erreur : Le fichier '{input_file}' est introuvable.\")\n",
    "    exit()\n",
    "\n",
    "# Charger les donn√©es\n",
    "def load_data(input_file):\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        data = [line.strip() for line in f.readlines()]\n",
    "    \n",
    "    print(f\"‚úÖ {len(data)} lignes charg√©es depuis '{input_file}'\")\n",
    "    return data\n",
    "\n",
    "# Fonction de pr√©traitement (tokenisation, stopwords, stemming)\n",
    "def process(input_text):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    stop_words = set(stopwords.words('english'))  # Utilisation d'un set pour recherche rapide\n",
    "    \n",
    "    tokens = tokenizer.tokenize(input_text.lower())\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    tokens_stemmed = [stemmer.stem(word) for word in tokens]\n",
    "    \n",
    "    return tokens_stemmed\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Charger et traiter les donn√©es\n",
    "    data = load_data(input_file)\n",
    "    tokens = [process(text) for text in data]\n",
    "\n",
    "    # Cr√©ation du dictionnaire et de la matrice terme-document\n",
    "    dict_tokens = corpora.Dictionary(tokens)\n",
    "    doc_term_mat = [dict_tokens.doc2bow(token) for token in tokens]\n",
    "\n",
    "    # Nombre de sujets √† identifier\n",
    "    num_topics = 2\n",
    "\n",
    "    # Cr√©ation du mod√®le LDA\n",
    "    print(\"üîÑ Entra√Ænement du mod√®le LDA...\")\n",
    "    ldamodel = models.LdaModel(doc_term_mat, num_topics=num_topics, id2word=dict_tokens, passes=25)\n",
    "    print(\"‚úÖ Mod√®le LDA entra√Æn√© !\")\n",
    "\n",
    "    # Affichage des sujets avec leurs mots-cl√©s\n",
    "    num_words = 5\n",
    "    print(f\"\\nüìå Top {num_words} mots cl√©s pour chaque sujet :\")\n",
    "    topics = ldamodel.show_topics(num_topics=num_topics, num_words=num_words, formatted=False)\n",
    "\n",
    "    for topic_num, word_weights in topics:\n",
    "        print(f\"\\nüü¢ Sujet {topic_num + 1}:\")\n",
    "        for word, weight in word_weights:\n",
    "            print(f\"  {word} ==> {round(weight * 100, 2)}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce code effectue une analyse th√©matique sur un fichier texte (data.txt) en utilisant le mod√®le LDA (Latent Dirichlet Allocation) de Gensim. Il commence par charger le fichier et applique un pr√©traitement du texte, incluant la tokenisation, la suppression des stopwords et le stemming. Ensuite, il construit un dictionnaire de mots et une matrice terme-document pour pr√©parer les donn√©es. Il entra√Æne ensuite un mod√®le LDA avec deux sujets, puis affiche les cinq mots les plus repr√©sentatifs pour chaque sujet. Ce type d‚Äôanalyse est couramment utilis√© en NLP pour extraire des th√®mes cach√©s dans de grands corpus de texte."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
