{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\smain\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\smain\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\smain\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: click in c:\\users\\smain\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: tqdm in c:\\users\\smain\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\smain\\appdata\\roaming\\python\\python310\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.3; however, version 25.0.1 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\smain\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\smain\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (4.3.3)\n",
      "Requirement already satisfied: numpy<2.0,>=1.18.5 in c:\\users\\smain\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gensim) (1.26.4)\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in c:\\users\\smain\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gensim) (1.13.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\smain\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gensim) (7.1.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\smain\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.3; however, version 25.0.1 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\smain\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pattern in c:\\users\\smain\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.0.1a0)\n",
      "Requirement already satisfied: seaborn>=0.10.0 in c:\\users\\smain\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pattern) (0.13.2)\n",
      "Requirement already satisfied: scikit-learn>=1.5.0 in c:\\users\\smain\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pattern) (1.6.1)\n",
      "Requirement already satisfied: numpy>=1.26.4 in c:\\users\\smain\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pattern) (1.26.4)\n",
      "Requirement already satisfied: matplotlib>=3.9.0 in c:\\users\\smain\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pattern) (3.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\smain\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib>=3.9.0->pattern) (1.4.8)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\smain\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib>=3.9.0->pattern) (3.2.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\smain\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib>=3.9.0->pattern) (24.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\smain\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib>=3.9.0->pattern) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\smain\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib>=3.9.0->pattern) (0.12.1)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\smain\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib>=3.9.0->pattern) (11.1.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\smain\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib>=3.9.0->pattern) (4.56.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\smain\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib>=3.9.0->pattern) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\smain\\appdata\\roaming\\python\\python310\\site-packages (from python-dateutil>=2.7->matplotlib>=3.9.0->pattern) (1.17.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\smain\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn>=1.5.0->pattern) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\smain\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn>=1.5.0->pattern) (3.5.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\smain\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn>=1.5.0->pattern) (1.13.1)\n",
      "Requirement already satisfied: pandas>=1.2 in c:\\users\\smain\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from seaborn>=0.10.0->pattern) (2.2.3)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\smain\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas>=1.2->seaborn>=0.10.0->pattern) (2025.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\smain\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas>=1.2->seaborn>=0.10.0->pattern) (2025.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.3; however, version 25.0.1 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\smain\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sentence tokenizer:\n",
      "['Do you know how tokenization works?', \"It's actually quite interesting!\", \"Let's analyze a couple of sentences and figure it out.\"]\n",
      "\n",
      "Word tokenizer:\n",
      "['Do', 'you', 'know', 'how', 'tokenization', 'works', '?', 'It', \"'s\", 'actually', 'quite', 'interesting', '!', 'Let', \"'s\", 'analyze', 'a', 'couple', 'of', 'sentences', 'and', 'figure', 'it', 'out', '.']\n",
      "\n",
      "Word punct tokenizer:\n",
      "['Do', 'you', 'know', 'how', 'tokenization', 'works', '?', 'It', \"'\", 's', 'actually', 'quite', 'interesting', '!', 'Let', \"'\", 's', 'analyze', 'a', 'couple', 'of', 'sentences', 'and', 'figure', 'it', 'out', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, \\\n",
    "        word_tokenize, WordPunctTokenizer\n",
    "\n",
    "# Define input text\n",
    "input_text = \"Do you know how tokenization works? It's actually quite interesting! Let's analyze a couple of sentences and figure it out.\" \n",
    "\n",
    "# Sentence tokenizer \n",
    "print(\"\\nSentence tokenizer:\")\n",
    "print(sent_tokenize(input_text))\n",
    "\n",
    "# Word tokenizer\n",
    "print(\"\\nWord tokenizer:\")\n",
    "print(word_tokenize(input_text))\n",
    "\n",
    "# WordPunct tokenizer\n",
    "print(\"\\nWord punct tokenizer:\")\n",
    "print(WordPunctTokenizer().tokenize(input_text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce code utilise NLTK pour découper un texte en phrases et en mots. Il commence par segmenter le texte en phrases avec sent_tokenize. Ensuite, il divise le texte en mots en tenant compte des apostrophes et de la ponctuation avec word_tokenize. Enfin, il applique WordPunctTokenizer, qui sépare les mots et la ponctuation de manière plus stricte. Ce processus est utilisé en traitement automatique du langage pour analyser et structurer du texte.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "       INPUT WORD          PORTER       LANCASTER        SNOWBALL \n",
      " ====================================================================\n",
      "         writing           write            writ           write\n",
      "          calves            calv            calv            calv\n",
      "              be              be              be              be\n",
      "         branded           brand           brand           brand\n",
      "           horse            hors            hors            hors\n",
      "       randomize          random          random          random\n",
      "        possibly         possibl            poss         possibl\n",
      "       provision          provis          provid          provis\n",
      "        hospital          hospit          hospit          hospit\n",
      "            kept            kept            kept            kept\n",
      "        scratchy        scratchi        scratchy        scratchi\n",
      "            code            code             cod            code\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "input_words = ['writing', 'calves', 'be', 'branded', 'horse', 'randomize', \n",
    "        'possibly', 'provision', 'hospital', 'kept', 'scratchy', 'code']\n",
    "\n",
    "# Create various stemmer objects\n",
    "porter = PorterStemmer()\n",
    "lancaster = LancasterStemmer()\n",
    "snowball = SnowballStemmer('english')\n",
    "\n",
    "# Create a list of stemmer names for display\n",
    "stemmer_names = ['PORTER', 'LANCASTER', 'SNOWBALL']\n",
    "formatted_text = '{:>16}' * (len(stemmer_names) + 1)\n",
    "print('\\n', formatted_text.format('INPUT WORD', *stemmer_names), \n",
    "        '\\n', '='*68)\n",
    "\n",
    "# Stem each word and display the output\n",
    "for word in input_words:\n",
    "    output = [word, porter.stem(word), \n",
    "            lancaster.stem(word), snowball.stem(word)]\n",
    "    print(formatted_text.format(*output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce code applique trois algorithmes de stemming (Porter, Lancaster et Snowball) à une liste de mots en anglais. Il commence par créer des objets pour chaque algorithme de stemming, puis définit une liste de mots à traiter. Ensuite, il génère un tableau affichant chaque mot original et ses versions réduites selon chaque algorithme. Le stemming permet de réduire les mots à leur racine pour normaliser le texte en traitement automatique du langage.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "               INPUT WORD         NOUN LEMMATIZER         VERB LEMMATIZER \n",
      " ===========================================================================\n",
      "                 writing                 writing                   write\n",
      "                  calves                    calf                   calve\n",
      "                      be                      be                      be\n",
      "                 branded                 branded                   brand\n",
      "                   horse                   horse                   horse\n",
      "               randomize               randomize               randomize\n",
      "                possibly                possibly                possibly\n",
      "               provision               provision               provision\n",
      "                hospital                hospital                hospital\n",
      "                    kept                    kept                    keep\n",
      "                scratchy                scratchy                scratchy\n",
      "                    code                    code                    code\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "input_words = ['writing', 'calves', 'be', 'branded', 'horse', 'randomize', \n",
    "        'possibly', 'provision', 'hospital', 'kept', 'scratchy', 'code']\n",
    "\n",
    "# Create lemmatizer object \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Create a list of lemmatizer names for display\n",
    "lemmatizer_names = ['NOUN LEMMATIZER', 'VERB LEMMATIZER']\n",
    "formatted_text = '{:>24}' * (len(lemmatizer_names) + 1)\n",
    "print('\\n', formatted_text.format('INPUT WORD', *lemmatizer_names), \n",
    "        '\\n', '='*75)\n",
    "\n",
    "# Lemmatize each word and display the output\n",
    "for word in input_words:\n",
    "    output = [word, lemmatizer.lemmatize(word, pos='n'),\n",
    "           lemmatizer.lemmatize(word, pos='v')]\n",
    "    print(formatted_text.format(*output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce code applique la lemmatisation à une liste de mots en utilisant WordNetLemmatizer de NLTK. Contrairement au stemming, la lemmatisation ramène un mot à sa forme lexicale correcte en fonction de sa catégorie grammaticale. Le script traite chaque mot sous deux formes : en tant que nom (pos='n') et en tant que verbe (pos='v'). Ensuite, il affiche un tableau comparant chaque mot avec ses versions lemmatisées. Ce processus est essentiel en traitement automatique du langage pour améliorer la précision de l'analyse sémantique.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of text chunks = 18 \n",
      "\n",
      "Chunk 1 ==> The Fulton County Grand Jury said Friday an invest\n",
      "Chunk 2 ==> '' . ( 2 ) Fulton legislators `` work with city of\n",
      "Chunk 3 ==> . Construction bonds Meanwhile , it was learned th\n",
      "Chunk 4 ==> , anonymous midnight phone calls and veiled threat\n",
      "Chunk 5 ==> Harris , Bexar , Tarrant and El Paso would be $451\n",
      "Chunk 6 ==> set it for public hearing on Feb. 22 . The proposa\n",
      "Chunk 7 ==> College . He has served as a border patrolman and \n",
      "Chunk 8 ==> of his staff were doing on the address involved co\n",
      "Chunk 9 ==> plan alone would boost the base to $5,000 a year a\n",
      "Chunk 10 ==> nursing homes In the area of `` community health s\n",
      "Chunk 11 ==> of its Angola policy prove harsh , there has been \n",
      "Chunk 12 ==> system which will prevent Laos from being used as \n",
      "Chunk 13 ==> reform in recipient nations . In Laos , the admini\n",
      "Chunk 14 ==> . He is not interested in being named a full-time \n",
      "Chunk 15 ==> said , `` to obtain the views of the general publi\n",
      "Chunk 16 ==> '' . Mr. Reama , far from really being retired , i\n",
      "Chunk 17 ==> making enforcement of minor offenses more effectiv\n",
      "Chunk 18 ==> to tell the people where he stands on the tax issu\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from nltk.corpus import brown\n",
    "\n",
    "# Split the input text into chunks, where\n",
    "# each chunk contains N words\n",
    "def chunker(input_data, N):\n",
    "    input_words = input_data.split(' ')\n",
    "    output = []\n",
    "\n",
    "    cur_chunk = []\n",
    "    count = 0\n",
    "    for word in input_words:\n",
    "        cur_chunk.append(word)\n",
    "        count += 1\n",
    "        if count == N:\n",
    "            output.append(' '.join(cur_chunk))\n",
    "            count, cur_chunk = 0, []\n",
    "\n",
    "    output.append(' '.join(cur_chunk))\n",
    "\n",
    "    return output \n",
    "\n",
    "if __name__=='__main__':\n",
    "    # Read the first 12000 words from the Brown corpus\n",
    "    input_data = ' '.join(brown.words()[:12000])\n",
    "\n",
    "    # Define the number of words in each chunk \n",
    "    chunk_size = 700\n",
    "\n",
    "    chunks = chunker(input_data, chunk_size)\n",
    "    print('\\nNumber of text chunks =', len(chunks), '\\n')\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        print('Chunk', i+1, '==>', chunk[:50])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce code divise un texte en segments de taille fixe en utilisant un chunking basé sur le nombre de mots. Il commence par extraire les 12 000 premiers mots du corpus Brown de NLTK, puis les segmente en morceaux contenant 700 mots chacun. Chaque chunk est ensuite stocké dans une liste et affiché avec ses 50 premiers caractères pour un aperçu. Ce type de segmentation est utile pour le traitement de texte en lots, notamment en NLP et apprentissage automatique.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Vocabulary:\n",
      " ['and' 'between' 'europe' 'formulate' 'have' 'in' 'mathematics' 'of'\n",
      " 'that' 'the']\n",
      "\n",
      "Document term matrix:\n",
      "\n",
      "         Word     Chunk-1     Chunk-2 \n",
      "\n",
      "         and           4           1\n",
      "     between           1           1\n",
      "      europe           1           1\n",
      "   formulate           1           1\n",
      "        have           1           1\n",
      "          in           6           1\n",
      " mathematics           1           1\n",
      "          of           5           3\n",
      "        that           2           1\n",
      "         the           9           1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Fonction pour lire le fichier\n",
    "def read_file(filepath):\n",
    "    with open(filepath, 'r', encoding='utf-8') as file:\n",
    "        return file.read()\n",
    "\n",
    "# Fonction pour diviser le texte en segments\n",
    "def chunker(text, chunk_size):\n",
    "    return [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "\n",
    "# Charger le texte depuis `data.txt`\n",
    "file_path = r\"C:\\Users\\smain\\OneDrive\\Documents\\data.txt\"  # Ajouter le `r` pour éviter les problèmes d'échappement\n",
    "input_data = read_file(file_path)\n",
    "\n",
    "# Nombre de mots dans chaque chunk\n",
    "chunk_size = 800\n",
    "text_chunks = chunker(input_data, chunk_size)\n",
    "\n",
    "# Convertir les segments en dictionnaire\n",
    "chunks = [{'index': i, 'text': chunk} for i, chunk in enumerate(text_chunks)]\n",
    "\n",
    "# Extraction de la matrice terme-document\n",
    "count_vectorizer = CountVectorizer(min_df=2, max_df=20)  # Ajustez min_df si besoin\n",
    "document_term_matrix = count_vectorizer.fit_transform([chunk['text'] for chunk in chunks])\n",
    "\n",
    "# Extraction du vocabulaire\n",
    "vocabulary = np.array(count_vectorizer.get_feature_names_out())\n",
    "print(\"\\nVocabulary:\\n\", vocabulary)\n",
    "\n",
    "# Générer les noms de chunks\n",
    "chunk_names = [f'Chunk-{i+1}' for i in range(len(text_chunks))]\n",
    "\n",
    "# Affichage de la matrice terme-document\n",
    "print(\"\\nDocument term matrix:\")\n",
    "formatted_text = '{:>12}' * (len(chunk_names) + 1)\n",
    "print('\\n', formatted_text.format('Word', *chunk_names), '\\n')\n",
    "\n",
    "for word, item in zip(vocabulary, document_term_matrix.T):\n",
    "    output = [word] + [str(freq) for freq in item.data]\n",
    "    print(formatted_text.format(*output))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce code lit un fichier texte (data.txt), le divise en segments de 800 caractères, puis crée une matrice terme-document en utilisant CountVectorizer de sklearn. Il commence par charger le texte, le segmente en chunks, et construit un dictionnaire contenant ces segments. Ensuite, il extrait les termes fréquents (présents dans au moins 2 et au plus 20 segments) et génère un vocabulaire. Enfin, il affiche la matrice terme-document qui représente la fréquence des mots dans chaque chunk. Ce procédé est utile en analyse de texte et NLP pour identifier les termes les plus significatifs d'un document volumineux.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Fetching training data...\n",
      "✅ Training data loaded. Number of documents: 2844\n",
      "🛠 Vectorizing text data...\n",
      "✅ Training data vectorized. Shape: (2844, 40018)\n",
      "🔁 Transforming with TF-IDF...\n",
      "✅ TF-IDF transformation done. Shape: (2844, 40018)\n",
      "🤖 Training Naïve Bayes model...\n",
      "✅ Model trained successfully!\n",
      "📡 Transforming input data...\n",
      "🔮 Predicting categories...\n",
      "\n",
      "🎯 Predictions:\n",
      "🔹 Input: You need to be careful with cars when you are driving on slippery roads\n",
      "   Predicted category: Autos\n",
      "\n",
      "🔹 Input: A lot of devices can be operated wirelessly\n",
      "   Predicted category: Electronics\n",
      "\n",
      "🔹 Input: Players need to be careful when they are close to goal posts\n",
      "   Predicted category: Hockey\n",
      "\n",
      "🔹 Input: Political debates help us understand the perspectives of both sides\n",
      "   Predicted category: Medicine\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n",
    "\n",
    "# Définition des catégories\n",
    "category_map = {\n",
    "    'talk.politics.misc': 'Politics', \n",
    "    'rec.autos': 'Autos', \n",
    "    'rec.sport.hockey': 'Hockey', \n",
    "    'sci.electronics': 'Electronics', \n",
    "    'sci.med': 'Medicine'\n",
    "}\n",
    "\n",
    "print(\"🚀 Fetching training data...\")\n",
    "training_data = fetch_20newsgroups(subset='train', categories=category_map.keys(), shuffle=True, random_state=5)\n",
    "print(f\"✅ Training data loaded. Number of documents: {len(training_data.data)}\")\n",
    "\n",
    "# Vérification si les données sont bien récupérées\n",
    "if len(training_data.data) == 0:\n",
    "    print(\"❌ Aucune donnée récupérée. Vérifiez les catégories !\")\n",
    "    exit()\n",
    "\n",
    "# Vectorisation avec suppression des stopwords\n",
    "print(\"🛠 Vectorizing text data...\")\n",
    "count_vectorizer = CountVectorizer(stop_words='english')\n",
    "train_tc = count_vectorizer.fit_transform(training_data.data)\n",
    "print(f\"✅ Training data vectorized. Shape: {train_tc.shape}\")\n",
    "\n",
    "# Vérification si la vectorisation fonctionne\n",
    "if train_tc.shape[0] == 0 or train_tc.shape[1] == 0:\n",
    "    print(\"❌ La matrice de compte est vide. Problème de vectorisation !\")\n",
    "    exit()\n",
    "\n",
    "# Transformation TF-IDF\n",
    "print(\"🔁 Transforming with TF-IDF...\")\n",
    "tfidf = TfidfTransformer()\n",
    "train_tfidf = tfidf.fit_transform(train_tc)\n",
    "print(f\"✅ TF-IDF transformation done. Shape: {train_tfidf.shape}\")\n",
    "\n",
    "# Entraînement du modèle Naïve Bayes\n",
    "print(\"🤖 Training Naïve Bayes model...\")\n",
    "classifier = MultinomialNB().fit(train_tfidf, training_data.target)\n",
    "print(\"✅ Model trained successfully!\")\n",
    "\n",
    "# Données de test personnalisées\n",
    "input_data = [\n",
    "    'You need to be careful with cars when you are driving on slippery roads', \n",
    "    'A lot of devices can be operated wirelessly',\n",
    "    'Players need to be careful when they are close to goal posts',\n",
    "    'Political debates help us understand the perspectives of both sides'\n",
    "]\n",
    "\n",
    "# Transformation et prédiction\n",
    "print(\"📡 Transforming input data...\")\n",
    "input_tc = count_vectorizer.transform(input_data)\n",
    "input_tfidf = tfidf.transform(input_tc)\n",
    "\n",
    "print(\"🔮 Predicting categories...\")\n",
    "predictions = classifier.predict(input_tfidf)\n",
    "\n",
    "# Affichage des résultats\n",
    "print(\"\\n🎯 Predictions:\")\n",
    "for sent, category in zip(input_data, predictions):\n",
    "    print(f\"🔹 Input: {sent}\\n   Predicted category: {category_map[training_data.target_names[category]]}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce code entraîne un modèle Naïve Bayes multinomial pour classer des textes en fonction de leur contenu, en utilisant le dataset 20 Newsgroups. Il commence par récupérer des articles liés à cinq catégories (politique, automobile, hockey, électronique et médecine). Ensuite, il vectorise les textes en appliquant une transformation TF-IDF pour pondérer les mots les plus significatifs. Une fois le modèle entraîné, il est utilisé pour prédire la catégorie de nouveaux textes donnés en entrée. L'affichage final montre chaque phrase test avec sa catégorie prédite, ce qui est utile pour l'analyse de texte et la classification automatique.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of end letters: 1\n",
      "Accuracy = 74.7%\n",
      "Alexander ==> male\n",
      "Danielle ==> female\n",
      "David ==> male\n",
      "Cheryl ==> male\n",
      "\n",
      "Number of end letters: 2\n",
      "Accuracy = 78.79%\n",
      "Alexander ==> male\n",
      "Danielle ==> female\n",
      "David ==> male\n",
      "Cheryl ==> female\n",
      "\n",
      "Number of end letters: 3\n",
      "Accuracy = 77.22%\n",
      "Alexander ==> male\n",
      "Danielle ==> female\n",
      "David ==> male\n",
      "Cheryl ==> female\n",
      "\n",
      "Number of end letters: 4\n",
      "Accuracy = 69.98%\n",
      "Alexander ==> male\n",
      "Danielle ==> female\n",
      "David ==> male\n",
      "Cheryl ==> female\n",
      "\n",
      "Number of end letters: 5\n",
      "Accuracy = 64.63%\n",
      "Alexander ==> male\n",
      "Danielle ==> female\n",
      "David ==> male\n",
      "Cheryl ==> female\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "from nltk import NaiveBayesClassifier\n",
    "from nltk.classify import accuracy as nltk_accuracy\n",
    "from nltk.corpus import names\n",
    "\n",
    "# Extract last N letters from the input word\n",
    "# and that will act as our \"feature\"\n",
    "def extract_features(word, N=2):\n",
    "    last_n_letters = word[-N:]\n",
    "    return {'feature': last_n_letters.lower()}\n",
    "\n",
    "if __name__=='__main__':\n",
    "    # Create training data using labeled names available in NLTK\n",
    "    male_list = [(name, 'male') for name in names.words('male.txt')]\n",
    "    female_list = [(name, 'female') for name in names.words('female.txt')]\n",
    "    data = (male_list + female_list)\n",
    "\n",
    "    # Seed the random number generator\n",
    "    random.seed(5)\n",
    "\n",
    "    # Shuffle the data\n",
    "    random.shuffle(data)\n",
    "\n",
    "    # Create test data\n",
    "    input_names = ['Alexander', 'Danielle', 'David', 'Cheryl']\n",
    "\n",
    "    # Define the number of samples used for train and test\n",
    "    num_train = int(0.8 * len(data))\n",
    "\n",
    "    # Iterate through different lengths to compare the accuracy\n",
    "    for i in range(1, 6):\n",
    "        print('\\nNumber of end letters:', i)\n",
    "        features = [(extract_features(n, i), gender) for (n, gender) in data]\n",
    "        train_data, test_data = features[:num_train], features[num_train:]\n",
    "        classifier = NaiveBayesClassifier.train(train_data)\n",
    "\n",
    "        # Compute the accuracy of the classifier \n",
    "        accuracy = round(100 * nltk_accuracy(classifier, test_data), 2)\n",
    "        print('Accuracy = ' + str(accuracy) + '%')\n",
    "\n",
    "        # Predict outputs for input names using the trained classifier model\n",
    "        for name in input_names:\n",
    "            print(name, '==>', classifier.classify(extract_features(name, i)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce code utilise un classificateur Naïve Bayes pour prédire le genre d’un prénom en fonction de ses dernières lettres. Il commence par extraire les prénoms masculins et féminins du corpus names de NLTK, puis les mélange de manière aléatoire. Ensuite, il entraîne un modèle sur 80 % des données et teste sa précision sur les 20 % restants. Il répète ce processus pour des longueurs de suffixes allant de 1 à 5 lettres afin de comparer l’impact sur la précision. Enfin, il utilise le modèle entraîné pour prédire le genre de nouveaux prénoms, comme Alexander ou Danielle. Ce type de classification est souvent utilisé en NLP et analyse de données linguistiques.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of training datapoints: 1600\n",
      "Number of test datapoints: 400\n",
      "\n",
      "Accuracy of the classifier: 0.735\n",
      "\n",
      "Top 15 most informative words:\n",
      "1. outstanding\n",
      "2. insulting\n",
      "3. vulnerable\n",
      "4. ludicrous\n",
      "5. uninvolving\n",
      "6. astounding\n",
      "7. avoids\n",
      "8. fascination\n",
      "9. affecting\n",
      "10. animators\n",
      "11. anna\n",
      "12. darker\n",
      "13. seagal\n",
      "14. symbol\n",
      "15. idiotic\n",
      "\n",
      "Movie review predictions:\n",
      "\n",
      "Review: The costumes in this movie were great\n",
      "Predicted sentiment: Positive\n",
      "Probability: 0.59\n",
      "\n",
      "Review: I think the story was terrible and the characters were very weak\n",
      "Predicted sentiment: Negative\n",
      "Probability: 0.8\n",
      "\n",
      "Review: People say that the director of the movie is amazing\n",
      "Predicted sentiment: Positive\n",
      "Probability: 0.6\n",
      "\n",
      "Review: This is such an idiotic movie. I will not recommend it to anyone.\n",
      "Predicted sentiment: Negative\n",
      "Probability: 0.87\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import movie_reviews \n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.classify.util import accuracy as nltk_accuracy\n",
    " \n",
    "# Extract features from the input list of words\n",
    "def extract_features(words):\n",
    "    return dict([(word, True) for word in words])\n",
    " \n",
    "if __name__=='__main__':\n",
    "    # Load the reviews from the corpus \n",
    "    fileids_pos = movie_reviews.fileids('pos')\n",
    "    fileids_neg = movie_reviews.fileids('neg')\n",
    "     \n",
    "    # Extract the features from the reviews\n",
    "    features_pos = [(extract_features(movie_reviews.words(\n",
    "            fileids=[f])), 'Positive') for f in fileids_pos]\n",
    "    features_neg = [(extract_features(movie_reviews.words(\n",
    "            fileids=[f])), 'Negative') for f in fileids_neg]\n",
    "     \n",
    "    # Define the train and test split (80% and 20%)\n",
    "    threshold = 0.8\n",
    "    num_pos = int(threshold * len(features_pos))\n",
    "    num_neg = int(threshold * len(features_neg))\n",
    "     \n",
    "     # Create training and training datasets\n",
    "    features_train = features_pos[:num_pos] + features_neg[:num_neg]\n",
    "    features_test = features_pos[num_pos:] + features_neg[num_neg:]  \n",
    "\n",
    "    # Print the number of datapoints used\n",
    "    print('\\nNumber of training datapoints:', len(features_train))\n",
    "    print('Number of test datapoints:', len(features_test))\n",
    "     \n",
    "    # Train a Naive Bayes classifier \n",
    "    classifier = NaiveBayesClassifier.train(features_train)\n",
    "    print('\\nAccuracy of the classifier:', nltk_accuracy(\n",
    "            classifier, features_test))\n",
    "\n",
    "    N = 15\n",
    "    print('\\nTop ' + str(N) + ' most informative words:')\n",
    "    for i, item in enumerate(classifier.most_informative_features()):\n",
    "        print(str(i+1) + '. ' + item[0])\n",
    "        if i == N - 1:\n",
    "            break\n",
    "\n",
    "    # Test input movie reviews\n",
    "    input_reviews = [\n",
    "        'The costumes in this movie were great', \n",
    "        'I think the story was terrible and the characters were very weak',\n",
    "        'People say that the director of the movie is amazing', \n",
    "        'This is such an idiotic movie. I will not recommend it to anyone.' \n",
    "    ]\n",
    "\n",
    "    print(\"\\nMovie review predictions:\")\n",
    "    for review in input_reviews:\n",
    "        print(\"\\nReview:\", review)\n",
    "\n",
    "        # Compute the probabilities\n",
    "        probabilities = classifier.prob_classify(extract_features(review.split()))\n",
    "\n",
    "        # Pick the maximum value\n",
    "        predicted_sentiment = probabilities.max()\n",
    "\n",
    "        # Print outputs\n",
    "        print(\"Predicted sentiment:\", predicted_sentiment)\n",
    "        print(\"Probability:\", round(probabilities.prob(predicted_sentiment), 2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce code entraîne un classificateur Naïve Bayes pour effectuer une analyse de sentiment sur des critiques de films. Il commence par charger les critiques positives et négatives du corpus movie_reviews de NLTK, puis extrait leurs caractéristiques en représentant chaque mot comme une caractéristique binaire (présent ou non). Ensuite, il divise les données en 80 % pour l'entraînement et 20 % pour le test. Une fois le modèle entraîné, il affiche sa précision et identifie les 15 mots les plus informatifs pour la classification. Enfin, il teste le modèle sur de nouvelles critiques de films et prédit leur sentiment positif ou négatif avec un score de probabilité. Ce type d'analyse est utilisé en NLP et opinion mining pour détecter les émotions dans les textes.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 10 lignes chargées depuis 'C:\\Users\\smain\\OneDrive\\Documents\\data.txt'\n",
      "🔄 Entraînement du modèle LDA...\n",
      "✅ Modèle LDA entraîné !\n",
      "\n",
      "📌 Top 5 mots clés pour chaque sujet :\n",
      "\n",
      "🟢 Sujet 1:\n",
      "  empir ==> 3.89%\n",
      "  mathemat ==> 3.89%\n",
      "  time ==> 2.78%\n",
      "  histor ==> 2.78%\n",
      "  peopl ==> 2.78%\n",
      "\n",
      "🟢 Sujet 2:\n",
      "  europ ==> 3.12%\n",
      "  cultur ==> 3.12%\n",
      "  formul ==> 3.12%\n",
      "  set ==> 1.88%\n",
      "  structur ==> 1.88%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\smain\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer  \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from gensim import models, corpora\n",
    "\n",
    "# Télécharger les stopwords si nécessaire\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Définition du fichier\n",
    "input_file = r\"C:\\Users\\smain\\OneDrive\\Documents\\data.txt\"  # Chemin absolu Windows\n",
    "\n",
    "# Vérifier si le fichier existe\n",
    "if not os.path.exists(input_file):\n",
    "    print(f\"❌ Erreur : Le fichier '{input_file}' est introuvable.\")\n",
    "    exit()\n",
    "\n",
    "# Charger les données\n",
    "def load_data(input_file):\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        data = [line.strip() for line in f.readlines()]\n",
    "    \n",
    "    print(f\"✅ {len(data)} lignes chargées depuis '{input_file}'\")\n",
    "    return data\n",
    "\n",
    "# Fonction de prétraitement (tokenisation, stopwords, stemming)\n",
    "def process(input_text):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    stop_words = set(stopwords.words('english'))  # Utilisation d'un set pour recherche rapide\n",
    "    \n",
    "    tokens = tokenizer.tokenize(input_text.lower())\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    tokens_stemmed = [stemmer.stem(word) for word in tokens]\n",
    "    \n",
    "    return tokens_stemmed\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Charger et traiter les données\n",
    "    data = load_data(input_file)\n",
    "    tokens = [process(text) for text in data]\n",
    "\n",
    "    # Création du dictionnaire et de la matrice terme-document\n",
    "    dict_tokens = corpora.Dictionary(tokens)\n",
    "    doc_term_mat = [dict_tokens.doc2bow(token) for token in tokens]\n",
    "\n",
    "    # Nombre de sujets à identifier\n",
    "    num_topics = 2\n",
    "\n",
    "    # Création du modèle LDA\n",
    "    print(\"🔄 Entraînement du modèle LDA...\")\n",
    "    ldamodel = models.LdaModel(doc_term_mat, num_topics=num_topics, id2word=dict_tokens, passes=25)\n",
    "    print(\"✅ Modèle LDA entraîné !\")\n",
    "\n",
    "    # Affichage des sujets avec leurs mots-clés\n",
    "    num_words = 5\n",
    "    print(f\"\\n📌 Top {num_words} mots clés pour chaque sujet :\")\n",
    "    topics = ldamodel.show_topics(num_topics=num_topics, num_words=num_words, formatted=False)\n",
    "\n",
    "    for topic_num, word_weights in topics:\n",
    "        print(f\"\\n🟢 Sujet {topic_num + 1}:\")\n",
    "        for word, weight in word_weights:\n",
    "            print(f\"  {word} ==> {round(weight * 100, 2)}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce code effectue une analyse thématique sur un fichier texte (data.txt) en utilisant le modèle LDA (Latent Dirichlet Allocation) de Gensim. Il commence par charger le fichier et applique un prétraitement du texte, incluant la tokenisation, la suppression des stopwords et le stemming. Ensuite, il construit un dictionnaire de mots et une matrice terme-document pour préparer les données. Il entraîne ensuite un modèle LDA avec deux sujets, puis affiche les cinq mots les plus représentatifs pour chaque sujet. Ce type d’analyse est couramment utilisé en NLP pour extraire des thèmes cachés dans de grands corpus de texte."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
