{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Labs 10\n",
    "## Smain Belghazi ING3\n",
    "\n",
    "Dans un premier temps je vais installer OpenCV et importer  le package cv2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\smain\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install numpy\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in c:\\users\\smain\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.11.0.86)\n",
      "Requirement already satisfied: numpy>=1.21.2 in c:\\users\\smain\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from opencv-python) (2.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install opencv-python\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import cv2\n",
    "\n",
    "# Compute the frame differences\n",
    "def frame_diff(prev_frame, cur_frame, next_frame):\n",
    "    # Difference between the current frame and the next frame\n",
    "    diff_frames_1 = cv2.absdiff(next_frame, cur_frame)\n",
    "\n",
    "    # Difference between the current frame and the previous frame\n",
    "    diff_frames_2 = cv2.absdiff(cur_frame, prev_frame)\n",
    "\n",
    "    return cv2.bitwise_and(diff_frames_1, diff_frames_2)\n",
    "\n",
    "# Define a function to get the current frame from the webcam\n",
    "def get_frame(cap, scaling_factor):\n",
    "    # Read the current frame from the video capture object\n",
    "    _, frame = cap.read()\n",
    "\n",
    "    # Resize the image\n",
    "    frame = cv2.resize(frame, None, fx=scaling_factor, \n",
    "            fy=scaling_factor, interpolation=cv2.INTER_AREA)\n",
    "\n",
    "    # Convert to grayscale\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "    return gray \n",
    "\n",
    "if __name__=='__main__':\n",
    "    # Define the video capture object\n",
    "    cap = cv2.VideoCapture(0)\n",
    "\n",
    "    # Define the scaling factor for the images\n",
    "    scaling_factor = 0.5\n",
    "    \n",
    "    # Grab the current frame\n",
    "    prev_frame = get_frame(cap, scaling_factor) \n",
    "\n",
    "    # Grab the next frame\n",
    "    cur_frame = get_frame(cap, scaling_factor) \n",
    "\n",
    "    # Grab the frame after that\n",
    "    next_frame = get_frame(cap, scaling_factor) \n",
    "\n",
    "    # Keep reading the frames from the webcam \n",
    "    # until the user hits the 'Esc' key\n",
    "    while True:\n",
    "        # Display the frame difference\n",
    "        cv2.imshow('Object Movement', frame_diff(prev_frame, \n",
    "                cur_frame, next_frame))\n",
    "\n",
    "        # Update the variables\n",
    "        prev_frame = cur_frame\n",
    "        cur_frame = next_frame \n",
    "\n",
    "        # Grab the next frame\n",
    "        next_frame = get_frame(cap, scaling_factor)\n",
    "\n",
    "        # Check if the user hit the 'Esc' key\n",
    "        key = cv2.waitKey(10)\n",
    "        if key == 27:\n",
    "            break\n",
    "\n",
    "    # Close all the windows\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le bloc de code ouvre une fenêtre qui capture les mouvements en utilisant la différence entre plusieurs frames successives. Il calcule le AND bitwise entre deux frames pour comparer pixel par pixel, et si une différence est détectée, elle est mise à jour et affichée. Cela permet de montrer les silhouettes des objets en mouvement dans la fenêtre\n",
    "![Capture d'écran](détécteur de mouv.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Define a function to get the current frame from the webcam\n",
    "def get_frame(cap, scaling_factor):\n",
    "    # Read the current frame from the video capture object\n",
    "    _, frame = cap.read()\n",
    "\n",
    "    # Resize the image\n",
    "    frame = cv2.resize(frame, None, fx=scaling_factor, \n",
    "            fy=scaling_factor, interpolation=cv2.INTER_AREA)\n",
    "\n",
    "    return frame\n",
    "\n",
    "if __name__=='__main__':\n",
    "    # Define the video capture object\n",
    "    cap = cv2.VideoCapture(0)\n",
    "\n",
    "    # Define the scaling factor for the images\n",
    "    scaling_factor = 0.5\n",
    "\n",
    "    # Keep reading the frames from the webcam \n",
    "    # until the user hits the 'Esc' key\n",
    "    while True:\n",
    "        # Grab the current frame\n",
    "        frame = get_frame(cap, scaling_factor) \n",
    "\n",
    "        # Convert the image to HSV colorspace\n",
    "        hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "        # Define range of skin color in HSV\n",
    "        # Définir une nouvelle plage de couleurs pour mieux correspondre à ta peau\n",
    "        lower = np.array([0, 40, 50])   # Plage basse (peut-être ajustée)\n",
    "        upper = np.array([20, 150, 255]) # Plage haute (ajuster selon le besoin)\n",
    "\n",
    "\n",
    "        # Threshold the HSV image to get only skin color\n",
    "        mask = cv2.inRange(hsv, lower, upper)\n",
    "\n",
    "        # Bitwise-AND between the mask and original image\n",
    "        img_bitwise_and = cv2.bitwise_and(frame, frame, mask=mask)\n",
    "\n",
    "        # Run median blurring\n",
    "        img_median_blurred = cv2.medianBlur(img_bitwise_and, 5)\n",
    "\n",
    "        # Display the input and output\n",
    "        cv2.imshow('Input', frame)\n",
    "        cv2.imshow('Output', img_median_blurred)\n",
    "\n",
    "        # Check if the user hit the 'Esc' key\n",
    "        c = cv2.waitKey(5) \n",
    "        if c == 27:\n",
    "            break\n",
    "\n",
    "    # Close all the windows\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans le bloc de code précédent, le programme capture les images de la webcam et détecte une couleur spécifique (comme la peau) à partir des pixels de l'image. D'abord, une plage de couleurs est définie en utilisant l'espace de couleurs HSV. Ensuite, un masque est créé pour isoler cette couleur spécifique. Sur la deuxième frame, le programme applique un AND bitwise entre l'image d'origine et le masque pour ne conserver que la couleur de la peau détectée. Cela génère deux fenêtres : une montrant l'image originale et l'autre affichant uniquement la couleur choisie (par exemple, la peau).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Define a function to get the current frame from the webcam\n",
    "def get_frame(cap, scaling_factor):\n",
    "    # Read the current frame from the video capture object\n",
    "    _, frame = cap.read()\n",
    "\n",
    "    # Resize the image\n",
    "    frame = cv2.resize(frame, None, fx=scaling_factor, \n",
    "            fy=scaling_factor, interpolation=cv2.INTER_AREA)\n",
    "\n",
    "    return frame\n",
    "\n",
    "if __name__=='__main__':\n",
    "    # Define the video capture object\n",
    "    cap = cv2.VideoCapture(0)\n",
    "\n",
    "    # Define the background subtractor object\n",
    "    bg_subtractor = cv2.createBackgroundSubtractorMOG2()\n",
    "     \n",
    "    # Define the number of previous frames to use to learn. \n",
    "    # This factor controls the learning rate of the algorithm. \n",
    "    # The learning rate refers to the rate at which your model \n",
    "    # will learn about the background. Higher value for \n",
    "    # ‘history’ indicates a slower learning rate. You can \n",
    "    # play with this parameter to see how it affects the output.\n",
    "    history = 700\n",
    "\n",
    "    # Define the learning rate\n",
    "    learning_rate = 1.0/history\n",
    "\n",
    "    # Keep reading the frames from the webcam \n",
    "    # until the user hits the 'Esc' key\n",
    "    while True:\n",
    "        # Grab the current frame\n",
    "        frame = get_frame(cap, 0.5)\n",
    "\n",
    "        # Compute the mask \n",
    "        mask = bg_subtractor.apply(frame, learningRate=learning_rate)\n",
    "\n",
    "        # Convert grayscale image to RGB color image\n",
    "        mask = cv2.cvtColor(mask, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "        # Display the images\n",
    "        cv2.imshow('Input', frame)\n",
    "        cv2.imshow('Output', mask & frame)\n",
    "\n",
    "        # Check if the user hit the 'Esc' key\n",
    "        c = cv2.waitKey(10)\n",
    "        if c == 27:\n",
    "            break\n",
    "\n",
    "    # Release the video capture object\n",
    "    cap.release()\n",
    "    \n",
    "    # Close all the windows\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce code utilise la soustraction de fond (Background Subtraction) pour détecter les objets en mouvement dans une vidéo en direct depuis la webcam. Il capture chaque image, applique un soustracteur de fond MOG2 pour créer un masque qui isole les objets en mouvement, puis affiche à la fois l’image originale et l’image filtrée. L’apprentissage du fond est contrôlé par un paramètre history et un learning_rate ajustable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Define a class to handle object tracking related functionality\n",
    "class ObjectTracker(object):\n",
    "    def __init__(self, scaling_factor=0.5):\n",
    "        # Initialize the video capture object\n",
    "        self.cap = cv2.VideoCapture(0)\n",
    "\n",
    "        # Capture the frame from the webcam\n",
    "        _, self.frame = self.cap.read()\n",
    "\n",
    "        # Scaling factor for the captured frame\n",
    "        self.scaling_factor = scaling_factor\n",
    "\n",
    "        # Resize the frame\n",
    "        self.frame = cv2.resize(self.frame, None, \n",
    "                fx=self.scaling_factor, fy=self.scaling_factor, \n",
    "                interpolation=cv2.INTER_AREA)\n",
    "\n",
    "        # Create a window to display the frame\n",
    "        cv2.namedWindow('Object Tracker')\n",
    "\n",
    "        # Set the mouse callback function to track the mouse\n",
    "        cv2.setMouseCallback('Object Tracker', self.mouse_event)\n",
    "\n",
    "        # Initialize variable related to rectangular region selection\n",
    "        self.selection = None\n",
    "\n",
    "        # Initialize variable related to starting position \n",
    "        self.drag_start = None\n",
    "\n",
    "        # Initialize variable related to the state of tracking \n",
    "        self.tracking_state = 0\n",
    "\n",
    "    # Define a method to track the mouse events\n",
    "    def mouse_event(self, event, x, y, flags, param):\n",
    "        # Convert x and y coordinates into 16-bit numpy integers\n",
    "        x, y = np.int16([x, y]) \n",
    "\n",
    "        # Check if a mouse button down event has occurred\n",
    "        if event == cv2.EVENT_LBUTTONDOWN:\n",
    "            self.drag_start = (x, y)\n",
    "            self.tracking_state = 0\n",
    "\n",
    "        # Check if the user has started selecting the region\n",
    "        if self.drag_start:\n",
    "            if flags & cv2.EVENT_FLAG_LBUTTON:\n",
    "                # Extract the dimensions of the frame\n",
    "                h, w = self.frame.shape[:2]\n",
    "\n",
    "                # Get the initial position\n",
    "                xi, yi = self.drag_start\n",
    "\n",
    "                # Get the max and min values\n",
    "                x0, y0 = np.maximum(0, np.minimum([xi, yi], [x, y]))\n",
    "                x1, y1 = np.minimum([w, h], np.maximum([xi, yi], [x, y]))\n",
    "\n",
    "                # Reset the selection variable\n",
    "                self.selection = None\n",
    "\n",
    "                # Finalize the rectangular selection\n",
    "                if x1-x0 > 0 and y1-y0 > 0:\n",
    "                    self.selection = (x0, y0, x1, y1)\n",
    "\n",
    "            else:\n",
    "                # If the selection is done, start tracking  \n",
    "                self.drag_start = None\n",
    "                if self.selection is not None:\n",
    "                    self.tracking_state = 1\n",
    "\n",
    "    # Method to start tracking the object\n",
    "    def start_tracking(self):\n",
    "        # Iterate until the user presses the Esc key\n",
    "        while True:\n",
    "            # Capture the frame from webcam\n",
    "            _, self.frame = self.cap.read()\n",
    "            \n",
    "            # Resize the input frame\n",
    "            self.frame = cv2.resize(self.frame, None, \n",
    "                    fx=self.scaling_factor, fy=self.scaling_factor, \n",
    "                    interpolation=cv2.INTER_AREA)\n",
    "\n",
    "            # Create a copy of the frame\n",
    "            vis = self.frame.copy()\n",
    "\n",
    "            # Convert the frame to HSV colorspace\n",
    "            hsv = cv2.cvtColor(self.frame, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "            # Create the mask based on predefined thresholds\n",
    "            mask = cv2.inRange(hsv, np.array((0., 60., 32.)), \n",
    "                        np.array((180., 255., 255.)))\n",
    "\n",
    "            # Check if the user has selected the region\n",
    "            if self.selection:\n",
    "                # Extract the coordinates of the selected rectangle\n",
    "                x0, y0, x1, y1 = self.selection\n",
    "\n",
    "                # Extract the tracking window\n",
    "                self.track_window = (x0, y0, x1-x0, y1-y0)\n",
    "\n",
    "                # Extract the regions of interest \n",
    "                hsv_roi = hsv[y0:y1, x0:x1]\n",
    "                mask_roi = mask[y0:y1, x0:x1]\n",
    "\n",
    "                # Compute the histogram of the region of \n",
    "                # interest in the HSV image using the mask\n",
    "                hist = cv2.calcHist( [hsv_roi], [0], mask_roi, \n",
    "                        [16], [0, 180] )\n",
    "\n",
    "                # Normalize and reshape the histogram\n",
    "                cv2.normalize(hist, hist, 0, 255, cv2.NORM_MINMAX);\n",
    "                self.hist = hist.reshape(-1)\n",
    "\n",
    "                # Extract the region of interest from the frame\n",
    "                vis_roi = vis[y0:y1, x0:x1]\n",
    "\n",
    "                # Compute the image negative (for display only)\n",
    "                cv2.bitwise_not(vis_roi, vis_roi)\n",
    "                vis[mask == 0] = 0\n",
    "\n",
    "            # Check if the system in the \"tracking\" mode\n",
    "            if self.tracking_state == 1:\n",
    "                # Reset the selection variable\n",
    "                self.selection = None\n",
    "                \n",
    "                # Compute the histogram back projection\n",
    "                hsv_backproj = cv2.calcBackProject([hsv], [0], \n",
    "                        self.hist, [0, 180], 1)\n",
    "\n",
    "                # Compute bitwise AND between histogram \n",
    "                # backprojection and the mask\n",
    "                hsv_backproj &= mask\n",
    "\n",
    "                # Define termination criteria for the tracker\n",
    "                term_crit = (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, \n",
    "                        10, 1)\n",
    "\n",
    "                # Apply CAMShift on 'hsv_backproj'\n",
    "                track_box, self.track_window = cv2.CamShift(hsv_backproj, \n",
    "                        self.track_window, term_crit)\n",
    "\n",
    "                # Draw an ellipse around the object\n",
    "                cv2.ellipse(vis, track_box, (0, 255, 0), 2)\n",
    "\n",
    "            # Show the output live video\n",
    "            cv2.imshow('Object Tracker', vis)\n",
    "\n",
    "            # Stop if the user hits the 'Esc' key\n",
    "            c = cv2.waitKey(5)\n",
    "            if c == 27:\n",
    "                break\n",
    "\n",
    "        # Close all the windows\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\t# Start the tracker\n",
    "    ObjectTracker().start_tracking()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce code implémente un suivi d'objet en temps réel à l'aide d'OpenCV et de l'algorithme CamShift. Il capture des images depuis la webcam, permet à l'utilisateur de sélectionner une région d'intérêt avec la souris, puis suit cet objet à travers les images suivantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Define a function to track the object\n",
    "def start_tracking():\n",
    "    # Initialize the video capture object\n",
    "    cap = cv2.VideoCapture(0)\n",
    "\n",
    "    # Define the scaling factor for the frames\n",
    "    scaling_factor = 0.5\n",
    "\n",
    "    # Number of frames to track\n",
    "    num_frames_to_track = 5\n",
    "\n",
    "    # Skipping factor\n",
    "    num_frames_jump = 2\n",
    "\n",
    "    # Initialize variables\n",
    "    tracking_paths = []\n",
    "    frame_index = 0\n",
    "\n",
    "    # Define tracking parameters\n",
    "    tracking_params = dict(winSize  = (11, 11), maxLevel = 2,\n",
    "            criteria = (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, \n",
    "                10, 0.03))\n",
    "\n",
    "    # Iterate until the user hits the 'Esc' key\n",
    "    while True:\n",
    "        # Capture the current frame\n",
    "        _, frame = cap.read()\n",
    "\n",
    "        # Resize the frame\n",
    "        frame = cv2.resize(frame, None, fx=scaling_factor, \n",
    "                fy=scaling_factor, interpolation=cv2.INTER_AREA)\n",
    "\n",
    "        # Convert to grayscale\n",
    "        frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Create a copy of the frame\n",
    "        output_img = frame.copy()\n",
    "\n",
    "        if len(tracking_paths) > 0:\n",
    "            # Get images\n",
    "            prev_img, current_img = prev_gray, frame_gray\n",
    "\n",
    "            # Organize the feature points\n",
    "            feature_points_0 = np.float32([tp[-1] for tp in \\\n",
    "                    tracking_paths]).reshape(-1, 1, 2)\n",
    "\n",
    "            # Compute optical flow\n",
    "            feature_points_1, _, _ = cv2.calcOpticalFlowPyrLK(\n",
    "                    prev_img, current_img, feature_points_0, \n",
    "                    None, **tracking_params)\n",
    "\n",
    "            # Compute reverse optical flow\n",
    "            feature_points_0_rev, _, _ = cv2.calcOpticalFlowPyrLK(\n",
    "                    current_img, prev_img, feature_points_1, \n",
    "                    None, **tracking_params)\n",
    "\n",
    "            # Compute the difference between forward and \n",
    "            # reverse optical flow\n",
    "            diff_feature_points = abs(feature_points_0 - \\\n",
    "                    feature_points_0_rev).reshape(-1, 2).max(-1)\n",
    "\n",
    "            # Extract the good points\n",
    "            good_points = diff_feature_points < 1\n",
    "\n",
    "            # Initialize variable\n",
    "            new_tracking_paths = []\n",
    "\n",
    "            # Iterate through all the good feature points \n",
    "            for tp, (x, y), good_points_flag in zip(tracking_paths, \n",
    "                        feature_points_1.reshape(-1, 2), good_points):\n",
    "                # If the flag is not true, then continue\n",
    "                if not good_points_flag:\n",
    "                    continue\n",
    "\n",
    "                # Append the X and Y coordinates and check if\n",
    "                # its length greater than the threshold\n",
    "                tp.append((x, y))\n",
    "                if len(tp) > num_frames_to_track:\n",
    "                    del tp[0]\n",
    "\n",
    "                new_tracking_paths.append(tp)\n",
    "\n",
    "                # Draw a circle around the feature points\n",
    "                cv2.circle(output_img, tuple(np.int32((x, y))), 3, (0, 255, 0), -1)\n",
    "\n",
    "\n",
    "            # Update the tracking paths\n",
    "            tracking_paths = new_tracking_paths\n",
    "\n",
    "            # Draw lines\n",
    "            cv2.polylines(output_img, [np.int32(tp) for tp in \\\n",
    "                    tracking_paths], False, (0, 150, 0))\n",
    "\n",
    "        # Go into this 'if' condition after skipping the \n",
    "        # right number of frames\n",
    "        if not frame_index % num_frames_jump:\n",
    "            # Create a mask and draw the circles\n",
    "            mask = np.zeros_like(frame_gray)\n",
    "            mask[:] = 255\n",
    "            for x, y in [np.int32(tp[-1]) for tp in tracking_paths]:\n",
    "                cv2.circle(mask, (x, y), 6, 0, -1)\n",
    "\n",
    "            # Compute good features to track\n",
    "            feature_points = cv2.goodFeaturesToTrack(frame_gray, \n",
    "                    mask = mask, maxCorners = 500, qualityLevel = 0.3, \n",
    "                    minDistance = 7, blockSize = 7) \n",
    "\n",
    "            # Check if feature points exist. If so, append them\n",
    "            # to the tracking paths\n",
    "            if feature_points is not None:\n",
    "                for x, y in np.float32(feature_points).reshape(-1, 2):\n",
    "                    tracking_paths.append([(x, y)])\n",
    "\n",
    "        # Update variables\n",
    "        frame_index += 1\n",
    "        prev_gray = frame_gray\n",
    "\n",
    "        # Display output\n",
    "        cv2.imshow('Optical Flow', output_img)\n",
    "\n",
    "        # Check if the user hit the 'Esc' key\n",
    "        c = cv2.waitKey(1)\n",
    "        if c == 27:\n",
    "            break\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\t# Start the tracker\n",
    "    start_tracking()\n",
    "\n",
    "    # Close all the windows\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Le code utilise OpenCV pour capturer une vidéo en temps réel via la webcam, Chaque image est redimensionnée et convertie en niveaux de gris.\n",
    "Il détecte des \"good features to track\" grâce à l'algorithme de Shi-Tomasi puis il applique l'algorithme d’optical flow de Lucas-Kanade pour suivre les points détectés sur plusieurs images consécutives.\n",
    " Il calcule un \"reverse optical flow\" pour valider les points suivis (en comparant le déplacement aller-retour).\n",
    "\n",
    " Affichage :\n",
    "\n",
    "Les points suivis sont représentés par des cercles verts.\n",
    "\n",
    "Des lignes vertes montrent la trajectoire des points suivis.\n",
    "\n",
    "Pour resumé ce programme détecte des points caractéristiques dans une vidéo en direct, suit leurs mouvements grâce à Lucas-Kanade et visualise ces déplacements avec des lignes vertes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in c:\\users\\smain\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (24.2)\n",
      "Collecting pip\n",
      "  Downloading pip-25.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Downloading pip-25.0-py3-none-any.whl (1.8 MB)\n",
      "   ---------------------------------------- 0.0/1.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.8/1.8 MB 49.4 MB/s eta 0:00:00\n",
      "Installing collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 24.2\n",
      "    Uninstalling pip-24.2:\n",
      "      Successfully uninstalled pip-24.2\n",
      "Successfully installed pip-25.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The scripts pip.exe, pip3.12.exe and pip3.exe are installed in 'c:\\Users\\smain\\AppData\\Local\\Programs\\Python\\Python312\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    }
   ],
   "source": [
    "!python -m pip install --upgrade pip\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: opencv-python in c:\\users\\smain\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (4.11.0.86)\n",
      "Requirement already satisfied: numpy>=1.21.2 in c:\\users\\smain\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from opencv-python) (2.2.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0\n",
      "[notice] To update, run: C:\\Users\\smain\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade opencv-python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c:\\Users\\smain\\Downloads\\haarcascade_frontalface_default.xml\n",
    "\n",
    "\"C:/Users/smain/Downloads/haarcascade_eye.xml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load the Haar cascade file\n",
    "face_cascade = cv2.CascadeClassifier('c:/Users/smain/Downloads/haarcascade_frontalface_default.xml')\n",
    "\n",
    "\n",
    "# Check if the cascade file has been loaded correctly\n",
    "if face_cascade.empty():\n",
    "\traise IOError('Unable to load the face cascade classifier xml file')\n",
    "\n",
    "# Initialize the video capture object\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Define the scaling factor\n",
    "scaling_factor = 0.5\n",
    "\n",
    "# Iterate until the user hits the 'Esc' key\n",
    "while True:\n",
    "    # Capture the current frame\n",
    "    _, frame = cap.read()\n",
    "\n",
    "    # Resize the frame\n",
    "    frame = cv2.resize(frame, None, \n",
    "            fx=scaling_factor, fy=scaling_factor, \n",
    "            interpolation=cv2.INTER_AREA)\n",
    "\n",
    "    # Convert to grayscale\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Run the face detector on the grayscale image\n",
    "    face_rects = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "\n",
    "    # Draw a rectangle around the face\n",
    "    for (x,y,w,h) in face_rects:\n",
    "        cv2.rectangle(frame, (x,y), (x+w,y+h), (0,255,0), 3)\n",
    "\n",
    "    # Display the output\n",
    "    cv2.imshow('Face Detector', frame)\n",
    "\n",
    "    # Check if the user hit the 'Esc' key\n",
    "    c = cv2.waitKey(1)\n",
    "    if c == 27:\n",
    "        break\n",
    "\n",
    "# Release the video capture object\n",
    "cap.release()\n",
    "\n",
    "# Close all the windows\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "le bloc de code précedent utilise OpenCV pour capturer une vidéo en direct via la webcam et détecter les visages en temps réel. Il charge un classificateur Haar pour la détection des visages, convertit chaque image capturée en niveaux de gris, puis applique l'algorithme de détection de visages. Lorsque des visages sont détectés, il dessine des rectangles verts autour d'eux.\n",
    "\n",
    "Note:\n",
    "J'ai rencontré un problème pour lancer le code. Le fichier haarcascade_frontalface_default.xml ne se chargeait pas correctement. J'ai donc décidé de le télécharger sur GitHub, puis de mettre le lien du fichier sur mon ordinateur : c:/Users/smain/Downloads/haarcascade_frontalface_default.xml."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load the Haar cascade files for face and eye\n",
    "face_cascade = cv2.CascadeClassifier('c:/Users/smain/Downloads/haarcascade_frontalface_default.xml')\n",
    "eye_cascade = cv2.CascadeClassifier(\"C:/Users/smain/Downloads/haarcascade_eye.xml\")\n",
    "\n",
    "# Check if the face cascade file has been loaded correctly\n",
    "if face_cascade.empty():\n",
    "\traise IOError('Unable to load the face cascade classifier xml file')\n",
    "\n",
    "# Check if the eye cascade file has been loaded correctly\n",
    "if eye_cascade.empty():\n",
    "\traise IOError('Unable to load the eye cascade classifier xml file')\n",
    "\n",
    "# Initialize the video capture object\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Define the scaling factor\n",
    "ds_factor = 0.5\n",
    "\n",
    "# Iterate until the user hits the 'Esc' key\n",
    "while True:\n",
    "    # Capture the current frame\n",
    "    _, frame = cap.read()\n",
    "\n",
    "    # Resize the frame\n",
    "    frame = cv2.resize(frame, None, fx=ds_factor, fy=ds_factor, interpolation=cv2.INTER_AREA)\n",
    "\n",
    "    # Convert to grayscale\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Run the face detector on the grayscale image\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "\n",
    "    # For each face that's detected, run the eye detector\n",
    "    for (x,y,w,h) in faces:\n",
    "        # Extract the grayscale face ROI\n",
    "        roi_gray = gray[y:y+h, x:x+w]\n",
    "\n",
    "        # Extract the color face ROI\n",
    "        roi_color = frame[y:y+h, x:x+w]\n",
    "\n",
    "        # Run the eye detector on the grayscale ROI\n",
    "        eyes = eye_cascade.detectMultiScale(roi_gray)\n",
    "\n",
    "        # Draw circles around the eyes\n",
    "        for (x_eye,y_eye,w_eye,h_eye) in eyes:\n",
    "            center = (int(x_eye + 0.5*w_eye), int(y_eye + 0.5*h_eye))\n",
    "            radius = int(0.3 * (w_eye + h_eye))\n",
    "            color = (0, 255, 0)\n",
    "            thickness = 3\n",
    "            cv2.circle(roi_color, center, radius, color, thickness)\n",
    "\n",
    "    # Display the output\n",
    "    cv2.imshow('Eye Detector', frame)\n",
    "\n",
    "    # Check if the user hit the 'Esc' key\n",
    "    c = cv2.waitKey(1)\n",
    "    if c == 27:\n",
    "        break\n",
    "\n",
    "# Release the video capture object\n",
    "cap.release()\n",
    "\n",
    "# Close all the windows\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Le bloc de code précédent utilise OpenCV pour capturer une vidéo en direct via la webcam et détecter les visages et les yeux en temps réel. Il charge les classificateurs Haar pour la détection des visages et des yeux, convertit chaque image capturée en niveaux de gris, puis applique l'algorithme de détection des visages. Une fois un visage détecté, il cherche également des yeux dans la région du visage. Lorsqu'il détecte des yeux, il dessine des cercles verts autour d'eux.\n",
    "\n",
    "Note:\n",
    "J'ai rencontré un problème pour lancer le code. Le fichier haarcascade_frontalface_default.xml ne se chargeait pas correctement. J'ai donc décidé de le télécharger sur GitHub, puis de mettre le lien du fichier sur mon ordinateur : c:/Users/smain/Downloads/haarcascade_frontalface_default.xml. mais egalement pour le fichier haarcascade_eye.xml "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
